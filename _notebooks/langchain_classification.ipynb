{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b43c572",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "- We have received 1000s of user queries at the customer support center.\n",
    "- Our objective is to classify the problem into 3 major categories so that we can address them on priority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f0531",
   "metadata": {},
   "source": [
    "# Solution Approach\n",
    "- Step 1: Take a sample from the queries received (`./data/customer_intent_raw.csv`) and ask the LLM to find 3 top categories for you. (Do multiple runs to find the most frequent ones.)\n",
    "\n",
    "- Step 2: Take another sample of data, label them manually using the 3 categories you found, label others where there is no match.\n",
    "\n",
    "- Step 3: Use the labeled sample data to evaluate which prompt technique you should use to categorize the query. (This will involve evaluating different prompt techniques.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b830695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utility.llm_factory import LLMFactory\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5315839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PropmtEvaluator class for evaluating and selecting the best prompt techniques\n",
    "class PropmtEvaluator:\n",
    "    _client = None  # Static variable for the LLM client\n",
    "    _user_message_template = \"\"\"```{description}```\"\"\"\n",
    "\n",
    "    _few_shot_system_message = \"\"\"Classify the following product desciption presented in the input into one of the following categories.\n",
    "        Categories - ['change order', 'track order', 'payment issue']\n",
    "        Product description will be delimited by triple backticks in the input.\n",
    "        Answer only 'change order', or 'track order', or 'payment issue'. Nothing Else. Do not explain your answer.\n",
    "    \"\"\"\n",
    "    _zero_shot_system_message = \"\"\"Classify the following product desciption presented in the input into one of the following categories.\n",
    "        Categories - ['change order', 'track order', 'payment issue', 'others']\n",
    "        Product description will be delimited by triple backticks in the input.\n",
    "        Answer only 'change order',\n",
    "        or 'track order', or 'payment issue',or 'others'. Nothing Else. Do not explain your answer.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __initialize():\n",
    "        \"\"\" Initialize the LLM client \"\"\"\n",
    "        client = LLMFactory.get_llm(\"groq\")\n",
    "        PropmtEvaluator._client = client\n",
    "\n",
    "    @staticmethod\n",
    "    def get_llm():\n",
    "        \"\"\" Retrieve the LLM client \"\"\"\n",
    "        if PropmtEvaluator._client is None:\n",
    "            PropmtEvaluator.__initialize()\n",
    "        return PropmtEvaluator._client\n",
    "\n",
    "    @staticmethod\n",
    "    def get_chain():\n",
    "        parser = StrOutputParser()\n",
    "        \"\"\" Retrieve the LLM pipeline chain \"\"\"\n",
    "        if PropmtEvaluator._client is None:\n",
    "            PropmtEvaluator.__initialize()\n",
    "        chain = PropmtEvaluator._client | parser \n",
    "        return chain\n",
    "\n",
    "    @staticmethod\n",
    "    def __evaluate_prompt(prompt, gold_examples, user_message_template, samples_to_output=5):\n",
    "        \"\"\"\n",
    "        Evaluate a given prompt based on its accuracy using the gold examples.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        model_predictions, ground_truths = [], []\n",
    "\n",
    "        # Iterate over the gold examples and make predictions\n",
    "        for example in json.loads(gold_examples):\n",
    "            gold_input = example['description']\n",
    "            user_input = [{'role':'user', 'content': user_message_template.format(description=gold_input)}]\n",
    "\n",
    "            try:\n",
    "                prediction = PropmtEvaluator.get_chain().invoke(prompt + user_input, config={\"temperature\": 0, \"max_tokens\": 4})\n",
    "                \n",
    "                while count < samples_to_output:\n",
    "                    count += 1\n",
    "                    print(f\"{count}-Original label: {example['task']} - Predicted label: {prediction}\")\n",
    "\n",
    "                model_predictions.append(prediction.strip().lower())  # Remove whitespace and lowercase output\n",
    "                ground_truths.append(example['task'].strip().lower())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # Calculate accuracy for each category\n",
    "        df = pd.DataFrame({'Predictions': model_predictions, 'Ground Truth': ground_truths})\n",
    "        labels = df['Ground Truth'].unique()\n",
    "\n",
    "        accuracy_df = pd.DataFrame(columns=labels)\n",
    "        for label in labels:\n",
    "            subset = df[df['Ground Truth'] == label]\n",
    "            accuracy = accuracy_score(subset['Ground Truth'], subset['Predictions'])\n",
    "            accuracy_df.loc[0, label] = accuracy\n",
    "\n",
    "        print(\"\\n\\n\", accuracy_df)\n",
    "        print(\"====================================================\")\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = accuracy_score(ground_truths, model_predictions)\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_examples(examples_df, n=4):\n",
    "        \"\"\"\n",
    "        Create randomized examples from each class.\n",
    "        \"\"\"\n",
    "        task1 = (examples_df.task == 'change order')\n",
    "        task2 = (examples_df.task == 'track order')\n",
    "        task3 = (examples_df.task == 'payment issue')\n",
    "        task4 = (examples_df.task == 'others')\n",
    "\n",
    "        t1_examples = examples_df.loc[task1, :].sample(n)\n",
    "        t2_examples = examples_df.loc[task2, :].sample(n)\n",
    "        t3_examples = examples_df.loc[task3, :].sample(n)\n",
    "        t4_examples = examples_df.loc[task4, :].sample(n)\n",
    "        examples = pd.concat([t1_examples, t2_examples, t3_examples, t4_examples])\n",
    "        randomized_examples = examples.sample(4*n, replace=False)\n",
    "\n",
    "        return randomized_examples.to_json(orient='records')\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_prompt(system_message, examples, user_message_template):\n",
    "        \"\"\"\n",
    "        Create a few-shot prompt for the classification task.\n",
    "        \"\"\"\n",
    "        few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
    "        for example in json.loads(examples):\n",
    "            few_shot_prompt.append({'role': 'user', 'content': user_message_template.format(description=example['description'])})\n",
    "            few_shot_prompt.append({'role': 'assistant', 'content': f\"{example['task']}\"})\n",
    "\n",
    "        return few_shot_prompt\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_zero_shot_promt(gold_examples):\n",
    "        \"\"\"\n",
    "        Evaluate zero-shot prompt on gold examples.\n",
    "        \"\"\"\n",
    "        zero_shot_prompt = [{'role':'system', 'content': PropmtEvaluator._zero_shot_system_message}]\n",
    "        token_size = LLMFactory.num_tokens_from_messages(zero_shot_prompt)\n",
    "        accuracy = PropmtEvaluator.__evaluate_prompt(zero_shot_prompt, gold_examples, PropmtEvaluator._user_message_template)\n",
    "        return token_size, accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_few_shot_promt(examples_df, gold_examples):\n",
    "        \"\"\"\n",
    "        Evaluate few-shot prompt on gold examples.\n",
    "        \"\"\"\n",
    "        few_shot_examples = PropmtEvaluator.__create_examples(examples_df, 2)\n",
    "        few_shot_prompt = PropmtEvaluator.__create_prompt(PropmtEvaluator._few_shot_system_message, few_shot_examples, PropmtEvaluator._user_message_template)\n",
    "        token_size = LLMFactory.num_tokens_from_messages(few_shot_prompt)\n",
    "        accuracy = PropmtEvaluator.__evaluate_prompt(few_shot_prompt, gold_examples, PropmtEvaluator._user_message_template)\n",
    "        return token_size, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7785917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classification class for processing queries and finding categories\n",
    "class Classification:\n",
    "    @staticmethod\n",
    "    def find_top_5_category():\n",
    "        # Load dataset containing user queries\n",
    "        dataset = pd.read_csv(\"../../../_data/customer_intent_raw.csv\")\n",
    "        raw_intent = dataset['description'].str.cat(sep=' || ')\n",
    "\n",
    "        # Create a prompt to extract problems and find the top 5 categories\n",
    "        prompt = f\"\"\"Extract the list of problems faced by the user from the following customer queries, separated by \" || \". \n",
    "                     A problem category should be a two word label. List out 10 unique problems.\n",
    "                     Then, identify the top 5 most frequent problems encountered by the user.\n",
    "\n",
    "                     Customer queries:\n",
    "                     {raw_intent}\"\"\"\n",
    "\n",
    "        # Create message for LLM\n",
    "        messages = [{'role':'user','content':prompt}]\n",
    "\n",
    "        # Get response from the LLM\n",
    "        response = PropmtEvaluator.get_chain().invoke(messages, config={\"temperature\": 0, \"seed\": 49})\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top 3 categories\n",
    "# Classification.find_top_5_category()\n",
    "\n",
    "# Step 2: Label data manually using top 3 categories found earlier.\n",
    "\n",
    "# Step 3: Evaluate prompt techniques for labeling\n",
    "dataset_df = pd.read_csv(\"../../../_data/customer_intent_labeled.csv\")\n",
    "examples_df, gold_examples_df = train_test_split(dataset_df, test_size=0.6, random_state=42)\n",
    "\n",
    "num_eval_runs = 1\n",
    "few_shot_performance = []\n",
    "\n",
    "# Perform evaluation multiple times with random data splits\n",
    "for _ in tqdm(range(num_eval_runs)):\n",
    "    gold_examples = (gold_examples_df.sample(100, random_state=42).to_json(orient='records'))\n",
    "    token_size, few_shot_accuracy = PropmtEvaluator.evaluate_few_shot_promt(examples_df, gold_examples)\n",
    "    few_shot_performance.append(few_shot_accuracy)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Few-shot performance: {few_shot_performance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
