{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55a0ee",
   "metadata": {},
   "source": [
    "# LangChain (LC) Building Blocks\n",
    "- Chat Models\n",
    "- Prompt Templates\n",
    "- Document Loaders\n",
    "- Vector Stors / dBs\n",
    "- Retriever\n",
    "- Chains\n",
    "- Runnable\n",
    "- Memory\n",
    "- Agents and Tools\n",
    "\n",
    "**langchain-core**: It contains the base abstractions for llms, embeddings, retrievers, vectorstores, document loaders, etc... It also includes the LangChain Expression Language (LCEL), which is a declarative runtime for composing ‚Äúrunnables‚Äù (sequences, DAGs, orchestration) with support for batch, async, streaming, fallback, etc.\n",
    "\n",
    "**langchain-community**: This package houses third-party integrations. All integrations are first built here in the langchain-community as beta code. Later, the integrations are packaged into their own. E.g. **langchain-openai** provides you the ChatOpenAI class which is also present in langchain-community.chat_models. \n",
    "\n",
    "**langchain**: Contains higher-level constructs: chains, agents, retrieval strategies, cognitive architectures.\n",
    "\n",
    "## Rule of Thumb (for selecting packages)\n",
    "\n",
    "| Package                   | Contains / Recommended Imports                                                                                                                                                                                                                                                                                                         | Notes                                                                                                                                  |\n",
    "| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`langchain-core`**      | **Modern LangChain primitives**: <br>‚Ä¢ PromptTemplate (`prompts`) <br>‚Ä¢ AIMessage / HumanMessage / SystemMessage (`messages`) <br>‚Ä¢ **Runnables** (`runnables`) <br>‚Ä¢ Output Parsers (`output_parsers`) <br>‚Ä¢ Schemas (`schema`) <br>‚Ä¢ **Modern Memory**: `BaseChatMessageHistory`, `ChatMessageHistory`, `RunnableWithMessageHistory` | üöÄ **New recommended foundation.**<br>Contains all modern building blocks for pipelines + memory. No integrations, no LLMs, no chains. |\n",
    "| **`langchain-community`** | Third-party & external integrations: <br>‚Ä¢ Vector stores (FAISS, Chroma, Pinecone, Milvus, etc.) <br>‚Ä¢ Embeddings (OpenAIEmbeddings, HuggingFaceEmbeddings) <br>‚Ä¢ Document loaders (PDF, Web, Notion, CSV, etc.) <br>‚Ä¢ Tools (Wikipedia, Arxiv, Browser, AzureSearch) <br>‚Ä¢ Retrievers (WikipediaRetriever, ArxivRetriever)            | üåç All community-maintained integrations. Required for *anything* that touches an external system or data source.                      |\n",
    "| **`langchain-openai`**    | OpenAI-specific components: <br>‚Ä¢ `ChatOpenAI` <br>‚Ä¢ `OpenAI` (completion API) <br>‚Ä¢ `OpenAIEmbeddings` <br>‚Ä¢ Moderation tools                                                                                                                                                                                                         | ü§ñ Official OpenAI provider package. Recommended for all OpenAI usage (instead of community).                                          |\n",
    "| **`langchain`**           | **Legacy high-level APIs**: <br>‚Ä¢ Chains (`LLMChain`, `RetrievalQA`, `ConversationalRetrievalChain`) <br>‚Ä¢ Agents (`initialize_agent`, old Agent types) <br>‚Ä¢ Tools (legacy agent tools) <br>‚Ä¢ **Legacy Memory**: `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationKGMemory`                                 | ‚ö†Ô∏è **Backward compatibility layer.** Still works but not recommended for new projects. Use `langchain-core` + Runnables instead.       |\n",
    "\n",
    "\n",
    "\n",
    "**langchain_text_splitters**: All document splitters have moved out of langchain-community to langchain_text_splitters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbfcadef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa434d",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b99080",
   "metadata": {},
   "source": [
    "### OpenAI Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0db490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "\n",
    "print(f\"answer is: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865efe6",
   "metadata": {},
   "source": [
    "### Groq Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c690c8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': {'message': 'Access denied. Please check your network settings.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionDeniedError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_groq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGroq\n\u001b[32m      3\u001b[39m llm = ChatGroq(model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.1-8b-instant\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33manswer is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:385\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> AIMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    383\u001b[39m         cast(\n\u001b[32m    384\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    395\u001b[39m         ).message,\n\u001b[32m    396\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1104\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1097\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1101\u001b[39m     **kwargs: Any,\n\u001b[32m   1102\u001b[39m ) -> LLMResult:\n\u001b[32m   1103\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:914\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    913\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m         )\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    922\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1208\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1212\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:571\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    567\u001b[39m params = {\n\u001b[32m    568\u001b[39m     **params,\n\u001b[32m    569\u001b[39m     **kwargs,\n\u001b[32m    570\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:464\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    246\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    304\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mPermissionDeniedError\u001b[39m: Error code: 403 - {'error': {'message': 'Access denied. Please check your network settings.'}}"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "\n",
    "print(f\"answer is: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d568bb",
   "metadata": {},
   "source": [
    "### Gemini Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9e87e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is: J'aime la programmation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(f\"answer is: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338eec4",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "There are 3 type messages in LC.\n",
    "- AIMessage\n",
    "- HumanMessage\n",
    "- SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3efedad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "AI Message looks like: content=\"J'adore programmer.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 22, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CdF5Q00vSvF2xo2LHnPAhApA6pk8f', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--d96f890f-4e14-4bdb-9d56-b800cee82e0b-0' usage_metadata={'input_tokens': 22, 'output_tokens': 6, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "# OR\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the user sentence to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(type(response))\n",
    "\n",
    "print(f\"AI Message looks like: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eae0d9",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "A prompt template in LC is a blueprint for dynamically creating prompts. Instead of hardcoding a string prompt you define a template with variables and Langchain is going to fill the variables at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5888cbc",
   "metadata": {},
   "source": [
    "### General Purpose Zero-shot Template\n",
    "\n",
    "- Zero-shot prompting is when you ask the model to do a task without giving any examples.\n",
    "- You just provide instructions about the task and the new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb8fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is: J'aime la programmation.\n"
     ]
    }
   ],
   "source": [
    "# A simplete general purpose prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=[\"language\", \"text\"],\n",
    "    template=\"Translate the text {text} in English to {language}.\",\n",
    ")\n",
    "\n",
    "response=llm.invoke(prompt.format(language=\"French\", text=\"I love programming.\"))\n",
    "\n",
    "print(f\"answer is: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb182a5",
   "metadata": {},
   "source": [
    "### Chat Template\n",
    "\n",
    "Designed specifically for chatting. It has the chat coversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01aed6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI is a combination of \"Genetic\" and \"AI\", which refers to the use of genetic algorithms in artificial intelligence. Genetic algorithms are a type of optimization technique inspired by the process of natural selection in biology. In simple terms, GenAI uses computer algorithms that mimic the process of evolution to solve complex problems and improve performance in AI systems. It works by generating a population of solutions, evaluating their fitness, selecting the best ones, and then using genetic operators like mutation and crossover to create new solutions. This process continues iteratively until an optimal solution is found.\n"
     ]
    }
   ],
   "source": [
    "# a chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize chat model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Chat prompt template with placeholders\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI assistant.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Explain {topic} in simple terms.\"),\n",
    "])\n",
    "\n",
    "response=llm.invoke(prompt.format_prompt(topic=\"genai\").to_messages())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f670df",
   "metadata": {},
   "source": [
    "### Few-Shot Prompt Template\n",
    "\n",
    "Few-shot prompting is a way to teach an AI model how to do a task by showing a few examples in the prompt itself.\n",
    "\n",
    "Instead of training the model, you just give it examples of what you want it to do, and then ask it to do the same for a new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e89e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She arrived on time to the meeting, even though there was a lot of traffic.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot prompting to simplify text inputs.\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize chat model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Template for each example\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"complex_text\", \"simple_text\"],\n",
    "    template=\"Original: {complex_text}\\nSimplified: {simple_text}\"\n",
    ")\n",
    "\n",
    "# Few-shot examples\n",
    "examples = [\n",
    "    {\"complex_text\": \"The weather today is extraordinarily inclement, with precipitation expected throughout the day.\", \"simple_text\": \"It's raining a lot today.\"},\n",
    "    {\"complex_text\": \"He expedited the process by employing an innovative methodology.\", \"simple_text\": \"He made it faster by using a new method.\"},\n",
    "    {\"complex_text\": \"The cat demonstrated remarkable agility while navigating the narrow ledge.\", \"simple_text\": \"The cat was very agile on the narrow ledge.\"},\n",
    "]\n",
    "\n",
    "# Few-shot prompt\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are an AI assistant that simplifies complex sentences into easy-to-understand English.\",\n",
    "    suffix=\"Original: {complex_text}\\nSimplified:\",\n",
    "    input_variables=[\"complex_text\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Format the prompt for a new input\n",
    "final_prompt = few_shot_prompt.format(complex_text=\"Despite the heavy traffic, she arrived at the meeting punctually.\")\n",
    "\n",
    "# Call the model\n",
    "response = llm.invoke(final_prompt)\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe7973",
   "metadata": {},
   "source": [
    "### Chain-Of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) is a prompting technique used with large language models where the AI is encouraged to reason step by step before giving the final answer.\n",
    "\n",
    "Instead of just outputting the answer, the AI explains its reasoning in intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9197d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, decide on the dates and budget for the trip. Next, research and choose accommodations in a convenient location. Then, create a list of must-see attractions and activities based on your interests. Finally, book any tickets or reservations in advance. So the answer is: Plan your weekend getaway by setting dates, booking accommodations, creating an itinerary, and making necessary reservations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize chat model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Template for each example\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Q: {question}\\nA: {answer}\"\n",
    ")\n",
    "\n",
    "# Few-shot examples with step-by-step reasoning\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"I want to visit Paris for 3 days. What should I do?\",\n",
    "        \"answer\": \"First, decide on top attractions: Eiffel Tower, Louvre, Montmartre. Next, plan each day to cover nearby locations efficiently. Then, check local transport and opening hours. Finally, book tickets in advance. So the answer is: Create a 3-day itinerary covering major sights, transport, and tickets.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I want to make a simple vegetarian dinner for 2 people.\",\n",
    "        \"answer\": \"First, decide on the main dish: maybe pasta or stir-fry. Next, select ingredients available at home. Then, plan cooking steps in order: prep, cook, assemble. Finally, season and serve. So the answer is: Cook a simple pasta or stir-fry following the prep and cook steps.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Few-shot CoT prompt\n",
    "cot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are an expert assistant. Answer questions step by step before giving a final recommendation.\",\n",
    "    suffix=\"Q: {question}\\nA:\",\n",
    "    input_variables=[\"question\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Format the prompt for a new question\n",
    "final_prompt = cot_prompt.format(question=\"I want to plan a weekend getaway to New York City.\")\n",
    "\n",
    "# Call the model\n",
    "response = llm.invoke(final_prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd5598",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10883a91",
   "metadata": {},
   "source": [
    "### Document Structure in LC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e806b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content='This is my document. It is the second document' metadata={'id': 2, 'source': 'The LangChain Papers', 'page': 2, 'author': 'John Doe', 'date': '2022-01-01', 'custom_field': 'foo'}\n",
      "content: This is my document. It is the second document\n",
      "metadata: {'id': 2, 'source': 'The LangChain Papers', 'page': 2, 'author': 'John Doe', 'date': '2022-01-01', 'custom_field': 'foo'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample Document Structure\n",
    "doc=Document(\n",
    "    page_content=\"This is my document. It is the second document\",\n",
    "    metadata={\n",
    "        \"id\": 2,\n",
    "        \"source\": \"The LangChain Papers\",\n",
    "        \"page\": 2,\n",
    "        \"author\": \"John Doe\",\n",
    "        \"date\": \"2022-01-01\",\n",
    "        \"custom_field\": \"foo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(type(doc))\n",
    "print(doc)\n",
    "print(f\"content: {doc.page_content}\")\n",
    "print(f\"metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2de9bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Loaded 1 documents\n",
      "Content preview: # üêç Introduction to Python Programming\n",
      "\n",
      "Python is one of the most popular and versatile programming \n",
      "Content preview: {'source': 'data/python.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "#loader\n",
    "loader = TextLoader('data/python.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "print(type(docs))\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Content preview: {docs[0].page_content[:100]}\")\n",
    "print(f\"Content preview: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2911717",
   "metadata": {},
   "source": [
    "## Vector dB / Stores\n",
    "- A Vector dB is the underlying database or storage system that can store vector embeddings. It does the heavy lifting of storing, indexing, and performing similarity search. E.g. FAISS, Pinecone, Weaviate, Chroma.\n",
    "\n",
    "- A Vector Store is the programmatic interface or wrapper you use in your code to interact with a vector DB. \n",
    "\n",
    "### Popular Open-Source Vector dB\n",
    "\n",
    "- **FAISS**: Is from facebook. It's in-memory dB and very fast.\n",
    "\n",
    "- **Chroma**: Is lightweight and easy-to-use dB for local projects.\n",
    "\n",
    "- **Qdrant**: Vector DB with built-in filters, supports metadata; easy REST/HTTP API.\n",
    "\n",
    "### Popular Managed/Cloud Vector dB\n",
    "\n",
    "- **Pinecone**: Fully managed, scalable, optimized for retrieval-augmented generation.\n",
    "\n",
    "- **Azure AI Search (formerly known as Cognitive Search)**: \n",
    "\n",
    "- **Azure Cosmos dB**: Cosmos DB now supports vector database features. Benefits: serverless, globally distributed, built-in vector + non-vector data.\n",
    "\n",
    "- **Azure Cache for Redis**: You can store embedding vectors in Redis on Azure and use vector similarity search. Useful if you already use Redis for caching and want to double-duty it for vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d58500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /home/azureuser/.local/lib/python3.10/site-packages (1.13.0)\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: langchain-text-splitters in /home/azureuser/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/azureuser/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /home/azureuser/.local/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-text-splitters) (1.0.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.43)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/lib/python3/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (5.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/azureuser/.local/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/azureuser/.local/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
      "Requirement already satisfied: anyio in /home/azureuser/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/azureuser/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /home/azureuser/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/azureuser/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/azureuser/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/azureuser/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/azureuser/.local/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.26.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/azureuser/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/azureuser/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FAISS is part of the langchain_community package\n",
    "# !pip uninstall faiss faiss-cpu faiss-gpu langchain_community -y\n",
    "!pip install --no-cache-dir faiss-cpu faiss-gpu langchain-text-splitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad1bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import faiss; print(faiss.__version__)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdb49d",
   "metadata": {},
   "source": [
    "### Build FAISS (in memory vector dB using HF Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33092467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 1606.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "---\n",
      "\n",
      "## üåê Learn More\n",
      "\n",
      "- [Google‚Äôs Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)  \n",
      "- [Coursera: Andrew Ng‚Äôs ML Course](https://www.coursera.org/learn/machine-learning)  \n",
      "- [Kaggle Learn](https://www.kaggle.com/learn)  \n",
      "- [scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)  \n",
      "\n",
      "---\n",
      "\n",
      "## üèÅ Conclusion\n",
      "\n",
      "Document 2:\n",
      "product budgets. It is also talking about the requirements, 5 plus years of experience. So it gives us requirements. Now if you want to change and let's say want to make it 2 plus years, you can always change those details. It is also talking about experience with agile development methodologies and product management tools such as Gira, Confluence, Viewer for a competitive salary and everything it is mentioning. Let me ask Chad Gipiti if it can create an onboarding program for the new wires in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # fixed\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader('./data/', glob=\"*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# üöÄ Use HuggingFaceEmbeddings to create the compatible Embeddings object\n",
    "# It takes the model_name and automatically loads it using SentenceTransformer\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'} # Optional: Specify 'cuda' if you have a GPU\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Query\n",
    "query = \"What is the main topic of the document?\"\n",
    "docs = vectorstore.similarity_search(query, k=2)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5a0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 matching chunks:\n",
      "\n",
      "[1] Content: ---\n",
      "\n",
      "## üß© Types of Machine Learning\n",
      "\n",
      "Machine Learning can be categorized into three main types:\n",
      "\n",
      "###...\n",
      "\n",
      "[1] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "=================================================================\n",
      "\n",
      "[2] Content: Examples:\n",
      "- Game-playing AI (like AlphaGo)\n",
      "- Robotics and autonomous driving\n",
      "\n",
      "Popular algorithms: **...\n",
      "\n",
      "[2] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "=================================================================\n",
      "\n",
      "[3] Content: ---\n",
      "\n",
      "## üöÄ Why Machine Learning?\n",
      "\n",
      "We live in a data-driven world. Every click, purchase, and interact...\n",
      "\n",
      "[3] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Test retrieval ---\n",
    "query = \"What are the type of machine learning fields?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "# results = chroma_db.similarity_search_with_score(query, k=3)\n",
    "print(\"\\nüîç Top 3 matching chunks:\")\n",
    "# results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"\\n[{i+1}] Metadata: {doc.metadata}...\")\n",
    "    # print(f\"\\n[{i+1}] Simillarity Score: {doc}...\")\n",
    "    print(f\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b702f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 matching chunks:\n",
      "\n",
      "[1] Content: ---\n",
      "\n",
      "## üß© Types of Machine Learning\n",
      "\n",
      "Machine Learning can be categorized into three main types:\n",
      "\n",
      "### 1. **Supervised Learning**\n",
      "The model is trained on labeled data ‚Äî meaning the correct answer is kno...\n",
      "\n",
      "[1] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "\n",
      "[1] Simillarity Score: 0.605402946472168...\n",
      "=================================================================\n",
      "\n",
      "[2] Content: Examples:\n",
      "- Game-playing AI (like AlphaGo)\n",
      "- Robotics and autonomous driving\n",
      "\n",
      "Popular algorithms: **Q-Learning, Deep Q-Network (DQN)**\n",
      "\n",
      "---\n",
      "\n",
      "## üí° How Machine Learning Works\n",
      "\n",
      "The general workflow of an...\n",
      "\n",
      "[2] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "\n",
      "[2] Simillarity Score: 0.7339668273925781...\n",
      "=================================================================\n",
      "\n",
      "[3] Content: ---\n",
      "\n",
      "## üöÄ Why Machine Learning?\n",
      "\n",
      "We live in a data-driven world. Every click, purchase, and interaction generates data. Machine Learning helps us extract **insights**, **detect trends**, and **automat...\n",
      "\n",
      "[3] Metadata: {'source': 'data/machine_learning.txt'}...\n",
      "\n",
      "[3] Simillarity Score: 0.8644626140594482...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Test retrieval ---\n",
    "query = \"What are the type of machine learning fields?\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "print(\"\\nüîç Top 3 matching chunks:\")\n",
    "# results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Content: {doc[0].page_content[:200]}...\")\n",
    "    print(f\"\\n[{i+1}] Metadata: {doc[0].metadata}...\")\n",
    "    print(f\"\\n[{i+1}] Simillarity Score: {doc[1]}...\")\n",
    "    print(f\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a300e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d02ca",
   "metadata": {},
   "source": [
    "### Build ChromadB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75505b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "to the need for anticipatory planning and governance.[11]\n",
      "Content Warning: This document contains content that some may Ô¨Ånd disturbing or oÔ¨Äensive,\n",
      "including content that is sexual, hateful, or violent in nature.\n",
      "41\n",
      "\n",
      "Document 2:\n",
      "as an art form,and they enjoy the value of lively discussions as well as\n",
      "disagreements.\n",
      "For them,arguments can be interesting and they can cover\n",
      "pretty much or any topic ---- as long as they occur in are respectful and\n",
      "intelligent manner.\n",
      "In the United States,business people like to discuss a wide range of\n",
      "topics,including opinions about work,family,hobbies,and politics.\n",
      "In\n",
      "Japan,China,and Korea,however,people are much more private.They do not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # fixed\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader('./data/', glob=\"*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", # Or whatever model you are using\n",
    "                                disallowed_special=() # This is the safest way to treat special tokens as normal text\n",
    "             )\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=\"./db/chroma_db\"\n",
    "                                    )\n",
    "\n",
    "# Query\n",
    "query = \"What is the main topic of the document?\"\n",
    "docs = vectorstore.max_marginal_relevance_search(query, k=2)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede1467",
   "metadata": {},
   "source": [
    "#### Load the dB from persistent store and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef822633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 matching chunks:\n",
      "\n",
      "[1] Content: structurally similar to metalearning as applied to ML in general. Here there is an extensive literat...\n",
      "\n",
      "[1] Metadata: {'page': 39, 'author': '', 'producer': 'pdfTeX-1.40.17', 'source': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'trapped': '', 'moddate': '2020-07-24T00:04:08+00:00', 'keywords': '', 'file_path': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'subject': '', 'format': 'PDF 1.5', 'total_pages': 75, 'creator': 'LaTeX with hyperref package', 'title': '', 'creationdate': '2020-07-24T00:04:08+00:00', 'creationDate': 'D:20200724000408Z', 'modDate': 'D:20200724000408Z'}...\n",
      "=================================================================\n",
      "\n",
      "[2] Content: been an important goal of Machine Learning research. Our work suggests that achieving signiÔ¨Åcant\n",
      "per...\n",
      "\n",
      "[2] Metadata: {'title': '', 'format': 'PDF 1.5', 'file_path': 'data/GPT-Architecture-2018.pdf', 'source': 'data/GPT-Architecture-2018.pdf', 'keywords': '', 'moddate': '2018-06-08T19:14:34+00:00', 'trapped': '', 'creationDate': 'D:20180608191434Z', 'subject': '', 'total_pages': 12, 'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-06-08T19:14:34+00:00', 'author': '', 'page': 7, 'modDate': 'D:20180608191434Z'}...\n",
      "=================================================================\n",
      "\n",
      "[3] Content: knowledge and ability to process long-range dependencies which are then successfully transferred to\n",
      "...\n",
      "\n",
      "[3] Metadata: {'moddate': '2018-06-08T19:14:34+00:00', 'trapped': '', 'author': '', 'source': 'data/GPT-Architecture-2018.pdf', 'file_path': 'data/GPT-Architecture-2018.pdf', 'modDate': 'D:20180608191434Z', 'keywords': '', 'creationDate': 'D:20180608191434Z', 'creationdate': '2018-06-08T19:14:34+00:00', 'title': '', 'page': 7, 'total_pages': 12, 'subject': '', 'producer': 'pdfTeX-1.40.18', 'format': 'PDF 1.5', 'creator': 'LaTeX with hyperref package'}...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Test retrieval ---\n",
    "vectorstore = Chroma(persist_directory=\"./db/chroma_db\", embedding_function=embeddings)\n",
    "query = \"What are the type of machine learning fields?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "# results = chroma_db.similarity_search_with_score(query, k=3)\n",
    "print(\"\\nüîç Top 3 matching chunks:\")\n",
    "# results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"\\n[{i+1}] Metadata: {doc.metadata}...\")\n",
    "    # print(f\"\\n[{i+1}] Simillarity Score: {doc}...\")\n",
    "    print(f\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305859f5",
   "metadata": {},
   "source": [
    "#### Max Marginal Relevance (MMR)\n",
    "Similarity search returns the top-k results that are closest to your query vector. It uses dot_prodct to find the close matched vectors. This sometimes could lead to strictly relevant passages.\n",
    "\n",
    "MMR returns result that are relevant to the query, but diverse from each other. It reduces redundancy and covers more topics / perspectives. It is great for QA, RAG, and summerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11e2e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 matching chunks:\n",
      "\n",
      "[1] Content: structurally similar to metalearning as applied to ML in general. Here there is an extensive literat...\n",
      "\n",
      "[1] Metadata: {'author': '', 'format': 'PDF 1.5', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200724000408Z', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': '', 'creator': 'LaTeX with hyperref package', 'file_path': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'moddate': '2020-07-24T00:04:08+00:00', 'page': 39, 'modDate': 'D:20200724000408Z', 'source': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'keywords': '', 'creationdate': '2020-07-24T00:04:08+00:00'}...\n",
      "=================================================================\n",
      "\n",
      "[2] Content: been an important goal of Machine Learning research. Our work suggests that achieving signiÔ¨Åcant\n",
      "per...\n",
      "\n",
      "[2] Metadata: {'page': 7, 'format': 'PDF 1.5', 'producer': 'pdfTeX-1.40.18', 'file_path': 'data/GPT-Architecture-2018.pdf', 'keywords': '', 'author': '', 'creationDate': 'D:20180608191434Z', 'trapped': '', 'total_pages': 12, 'modDate': 'D:20180608191434Z', 'moddate': '2018-06-08T19:14:34+00:00', 'creator': 'LaTeX with hyperref package', 'source': 'data/GPT-Architecture-2018.pdf', 'title': '', 'subject': '', 'creationdate': '2018-06-08T19:14:34+00:00'}...\n",
      "=================================================================\n",
      "\n",
      "[3] Content: pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625‚Äì660, 2010.\n",
      "[16] S...\n",
      "\n",
      "[3] Metadata: {'page': 8, 'format': 'PDF 1.5', 'creator': 'LaTeX with hyperref package', 'source': 'data/GPT-Architecture-2018.pdf', 'producer': 'pdfTeX-1.40.18', 'file_path': 'data/GPT-Architecture-2018.pdf', 'creationdate': '2018-06-08T19:14:34+00:00', 'moddate': '2018-06-08T19:14:34+00:00', 'subject': '', 'trapped': '', 'title': '', 'creationDate': 'D:20180608191434Z', 'modDate': 'D:20180608191434Z', 'keywords': '', 'author': '', 'total_pages': 12}...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", # Or whatever model you are using\n",
    "                                disallowed_special=() # This is the safest way to treat special tokens as normal text\n",
    "             )\n",
    "\n",
    "# --- Optional: Test retrieval ---\n",
    "vectorstore = Chroma(persist_directory=\"./db/chroma_db\", embedding_function=embeddings)\n",
    "query = \"What are the type of machine learning fields?\"\n",
    "results = vectorstore.max_marginal_relevance_search(query, k=3)\n",
    "print(\"\\nüîç Top 3 matching chunks:\")\n",
    "# results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"\\n[{i+1}] Metadata: {doc.metadata}...\")\n",
    "    print(f\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4d616",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them.\n",
    "\n",
    "Every vectorstore wrapper class (like faiss, FIASS) has their retriever class read document from it.\n",
    "\n",
    "There are many external retriever in the language_community package.\n",
    "\n",
    "- AzureAISearchRetriever\n",
    "- ArxivRetriever\n",
    "- TravilySearchAPIRetriever (used for internet search)\n",
    "- WikipediaRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170618aa",
   "metadata": {},
   "source": [
    "### Vector dB retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17cb5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Unsupervised pre-training\n",
      "Unsupervised pre-training is a special case of semi-supervised learning\n",
      "where the goal is to Ô¨Ånd a good initialization point instead of modifying the supervised learning\n",
      "objective. Early works explored the use of the technique in image classiÔ¨Åcation [20, 49, 63] and\n",
      "regression tasks [3]. Subsequent research [15] demonstrated that pre-training acts as a regularization\n",
      "scheme, enabling better generalization in deep neural networks. In recent work, the method has\n",
      "\n",
      "Document 1:\n",
      "{'trapped': '', 'title': '', 'moddate': '2018-06-08T19:14:34+00:00', 'creationdate': '2018-06-08T19:14:34+00:00', 'total_pages': 12, 'subject': '', 'modDate': 'D:20180608191434Z', 'author': '', 'page': 1, 'creator': 'LaTeX with hyperref package', 'creationDate': 'D:20180608191434Z', 'producer': 'pdfTeX-1.40.18', 'format': 'PDF 1.5', 'keywords': '', 'source': 'data/GPT-Architecture-2018.pdf', 'file_path': 'data/GPT-Architecture-2018.pdf'}\n",
      "\n",
      "Document 2:\n",
      "learning in natural language processing (NLP). Most deep learning methods require substantial\n",
      "amounts of manually labeled data, which restricts their applicability in many domains that suffer\n",
      "from a dearth of annotated resources [61]. In these situations, models that can leverage linguistic\n",
      "information from unlabeled data provide a valuable alternative to gathering more annotation, which\n",
      "can be time-consuming and expensive. Further, even in cases where considerable supervision\n",
      "\n",
      "Document 2:\n",
      "{'trapped': '', 'producer': 'pdfTeX-1.40.18', 'source': 'data/GPT-Architecture-2018.pdf', 'page': 0, 'modDate': 'D:20180608191434Z', 'subject': '', 'creator': 'LaTeX with hyperref package', 'moddate': '2018-06-08T19:14:34+00:00', 'creationDate': 'D:20180608191434Z', 'total_pages': 12, 'file_path': 'data/GPT-Architecture-2018.pdf', 'format': 'PDF 1.5', 'keywords': '', 'author': '', 'title': '', 'creationdate': '2018-06-08T19:14:34+00:00'}\n",
      "\n",
      "Document 3:\n",
      "learns new tasks from scratch at inference time or simply recognizes patterns seen during training ‚Äì this is an important issue which\n",
      "we discuss later in the paper, but ‚Äúmeta-learning‚Äù is intended to encompass both possibilities, and simply describes the inner-outer\n",
      "loop structure.\n",
      "4\n",
      "\n",
      "Document 3:\n",
      "{'total_pages': 75, 'file_path': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'author': '', 'moddate': '2020-07-24T00:04:08+00:00', 'modDate': 'D:20200724000408Z', 'creationdate': '2020-07-24T00:04:08+00:00', 'creator': 'LaTeX with hyperref package', 'page': 3, 'trapped': '', 'subject': '', 'creationDate': 'D:20200724000408Z', 'source': 'data/GPT-3-A Few Shot Learner-2020.pdf', 'producer': 'pdfTeX-1.40.17', 'title': '', 'format': 'PDF 1.5', 'keywords': ''}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create embeddings (must match the function used when the DB was created)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    "    disallowed_special=() \n",
    ")\n",
    "\n",
    "# üöÄ CORRECTED LINE: Use from_existing_persist_dir to load the database\n",
    "# Note: You still need the embedding_function to process the query before searching\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./db/chroma_db\", \n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3})\n",
    "\n",
    "# Retrieval\n",
    "docs = retriever.invoke(\"Explain supervised learning.\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")    \n",
    "    print(f\"Document {i+1}:\\n{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cbdda",
   "metadata": {},
   "source": [
    "### ArxivRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b142560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Document 1\n",
      "Title: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\n",
      "Authors: Luke Melas-Kyriazi\n",
      "Do You Even Need Attention? A Stack of Feed-Forward Layers Does\n",
      "Surprisingly Well on ImageNet\n",
      "Luke Melas-Kyriazi\n",
      "Oxford University\n",
      "lukemk@robots.ox.ac.uk\n",
      "Abstract\n",
      "The strong performance of vision transformers on im-\n",
      "age classiÔ¨Åcation and other vision tasks is often attributed\n",
      "to the design of their multi-head attention layers. How-\n",
      "ever, the extent to which attention is responsible for this\n",
      "strong performance remains unclear. In this short report,\n",
      "we ask: is the attention layer even necessary? S ...\n",
      "=================================================================\n",
      "\n",
      "üìÑ Document 2\n",
      "Title: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\n",
      "Authors: Serena Wang, Maya Gupta, Seungil You\n",
      "arXiv:1806.11202v1  [cs.LG]  28 Jun 2018\n",
      "Quit When You Can: EfÔ¨Åcient Evaluation of\n",
      "Ensembles with Ordering Optimization\n",
      "Serena Wang\n",
      "Google, Inc.\n",
      "serenawang@google.com\n",
      "Maya Gupta\n",
      "Google, Inc.\n",
      "mayagupta@google.com\n",
      "Seungil You\n",
      "Kakao Mobility\n",
      "sean.you@kakaomobility.com\n",
      "Abstract\n",
      "Given a classiÔ¨Åer ensemble and a set of examples to be classiÔ¨Åed, many examples\n",
      "may be conÔ¨Ådently and accurately classiÔ¨Åed after only a subset of the base models\n",
      "in the ensemble are evaluated. This can reduce both mean latenc ...\n",
      "=================================================================\n",
      "\n",
      "üìÑ Document 3\n",
      "Title: GAN Vocoder: Multi-Resolution Discriminator Is All You Need\n",
      "Authors: Jaeseong You, Dalhyun Kim, Gyuhyeon Nam, Geumbyeol Hwang, Gyeongsu Chae\n",
      "GAN Vocoder: Multi-Resolution Discriminator Is All You Need\n",
      "Jaeseong You, Dalhyun Kim, Gyuhyeon Nam, Geumbyeol Hwang, Gyeongsu Chae\n",
      "MoneyBrain Inc., Seoul, Korea\n",
      "{jaeseongyou, torch, ngh3053, goldstar, gc}@moneybrain.ai\n",
      "Abstract\n",
      "Several of the latest GAN-based vocoders show remarkable\n",
      "achievements, outperforming autoregressive and Ô¨Çow-based\n",
      "competitors in both qualitative and quantitative measures while\n",
      "synthesizing orders of magnitude faster. In this work, we hy-\n",
      "pothesize that the common facto ...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "# Create the retriever\n",
    "retriever = ArxivRetriever(\n",
    "    load_max_docs=3,        # number of papers to fetch\n",
    "    get_full_documents=True # fetch full PDF text when possible\n",
    ")\n",
    "\n",
    "# Retrieve documents using LangChain 0.2+ API (invoke)\n",
    "query = \"Attention is all you need.\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nüìÑ Document {i}\")\n",
    "    print(\"Title:\", doc.metadata.get(\"Title\"))\n",
    "    print(\"Authors:\", doc.metadata.get(\"Authors\"))\n",
    "    print(doc.page_content[:500], \"...\")  # print preview only\n",
    "    print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61842d8",
   "metadata": {},
   "source": [
    "### wikipedia Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c13d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Document 1\n",
      "Title: Supervised learning\n",
      "In machine learning, supervised learning (SL) is a type of machine learning paradigm where an algorithm learns to map input data to a specific output based on example input-output pairs. This process involves training a statistical model using labeled data, meaning each piece of input data is provided with the correct output. For instance, if you want a model to identify cats in images, supervised learning would involve feeding it many images of cats (inputs) that are explicitly labeled \"cat\" (o ...\n",
      "\n",
      "üìÑ Document 2\n",
      "Title: Explainable artificial intelligence\n",
      "Within artificial intelligence (AI), explainable AI (XAI), generally overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms. The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms, to make them more understandable and transparent. This addresses users' requirement to assess safety and scrutinize the automated de ...\n",
      "\n",
      "üìÑ Document 3\n",
      "Title: Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
      "ML finds application in many fields,  ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "# Create retriever\n",
    "retriever = WikipediaRetriever(\n",
    "    top_k_results=3,       # number of pages to return\n",
    "    doc_content_chars_max=4000  # limit size of each result\n",
    ")\n",
    "\n",
    "# Query using LangChain 0.2+ API\n",
    "query = \"Explain supervised learning\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nüìÑ Document {i}\")\n",
    "    print(\"Title:\", doc.metadata.get(\"title\"))\n",
    "    print(doc.page_content[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b7c65",
   "metadata": {},
   "source": [
    "## Chains and Runnables\n",
    "\n",
    "Chains are heart of the LC. It connects prompts, LLMs, tools, memory and output parser into reusable pipelines.\n",
    "\n",
    "### Type of Chains\n",
    "\n",
    "- LLMChain - single prompt and output\n",
    "\n",
    "- ConversationChain - used for chatbot\n",
    "\n",
    "- SequentialChain - multi-step workflows\n",
    "\n",
    "- SimpleSequentialChain\n",
    "\n",
    "- TransformChain\n",
    "\n",
    "- RetrievalQAChain\n",
    "\n",
    "- SQLDatabaseChain - Connects LLMs with SQL dBs\n",
    "\n",
    "- APIChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4711b2",
   "metadata": {},
   "source": [
    "### A Simple Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb71dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genai is like a magical genie in a bottle, but instead of granting wishes, it's a virtual assistant here to make your life easier! With its advanced AI technology, Genai can help you with tasks, answer questions, provide information, and even entertain you with jokes and fun facts. Think of Genai as your trusty sidekick in the digital world, always ready to assist you with a smile (well, virtually at least). So whether you need help organizing your schedule, finding the best pizza place in town, or just want someone to chat with, Genai is here to grant your digital wishes!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful AI assistant who response with humour.\"),\n",
    "        (\"human\", \"Explain {topic} in {no_of_words} words.\"),],\n",
    "    input_variables=[\"topic\", \"no_of_words\"],\n",
    ")\n",
    "\n",
    "#Define runnables\n",
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x).to_messages())\n",
    "format_prompt_with_llm = RunnableLambda(lambda x: llm.invoke(x))\n",
    "format_prompt_with_parser = RunnableLambda(lambda x: x.content)\n",
    "\n",
    "chain = RunnableSequence(first=format_prompt, middle=[format_prompt_with_llm], last=format_prompt_with_parser)\n",
    "\n",
    "# Alternatively, you can simply chain them using the | operator\n",
    "#chain = prompt_template | llm | parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"genai\", \"no_of_words\": 100})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e86b6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genai is like a magical genie in a bottle, but instead of granting wishes, it's a helpful AI assistant here to make your life easier! Genai can provide information, answer questions, offer suggestions, and even crack a joke or two. With its vast knowledge and quick wit, Genai is always ready to assist you with whatever you need. Just think of Genai as your trusty sidekick in the digital world, here to save the day and bring a smile to your face. So go ahead, ask Genai anything ‚Äì it's here to help in its own quirky way!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful AI assistant who response with humour.\"),\n",
    "        (\"human\", \"Explain {topic} in {no_of_words} words.\"),],\n",
    "    input_variables=[\"topic\", \"no_of_words\"],\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"genai\", \"no_of_words\": 100})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2f82c",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "\n",
    "Here you are sequencing 2 LLM calls. One call gets the detailed report from llm, the second call sends the detailed report to LLM asking to summerize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 5-point markdown summary of the text:\n",
      "\n",
      "*   **Transformative Potential:** AI is rapidly reshaping healthcare, promising revolutions in diagnosis, treatment, and prevention, leading to improved outcomes, reduced costs, and increased accessibility.\n",
      "*   **Diverse Applications:** AI is actively deployed across various domains, including advanced diagnostics (medical imaging, early prediction), accelerated drug discovery, personalized medicine, optimized clinical operations, remote patient monitoring, and surgical robotics.\n",
      "*   **Key Benefits:** Its integration offers significant advantages such as enhanced accuracy and efficiency, better patient outcomes, cost reduction, increased accessibility, accelerated innovation, and a shift towards proactive and preventative care.\n",
      "*   **Major Challenges:** Widespread adoption faces hurdles like poor data quality and bias, critical privacy and security concerns, complex regulatory frameworks, infrastructure limitations, and issues of trust and explainability (\"black box\" problem).\n",
      "*   **Ethical Imperatives & Future Outlook:** Addressing ethical considerations (algorithmic bias, accountability, transparency, human oversight) is paramount for responsible AI development, fostering human-AI collaboration, and ensuring a human-centric, equitable future for healthcare.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Generate a detailed report on {topic}.\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Generate a 5-point markdown summary for this text:\\n{text}.\")\n",
    "\n",
    "report_chain = prompt1 | llm | parser\n",
    "summary_chain = prompt2 | llm | parser\n",
    "\n",
    "pipeline = RunnableSequence(\n",
    "    first=report_chain,\n",
    "    middle=[RunnableLambda(lambda x: {\"text\": x})],\n",
    "    last=summary_chain\n",
    ")\n",
    "\n",
    "response = pipeline.invoke({\"topic\": \"AI impact on Healthcare\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64fbc791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a 5-point markdown summary of the text:\n",
      "\n",
      "*   **AI's Transformative Potential:** Artificial Intelligence is rapidly reshaping healthcare, promising revolutions in diagnosis, treatment, and prevention, aiming for improved outcomes, reduced costs, and increased accessibility.\n",
      "*   **Diverse Applications:** AI's impact spans critical areas including advanced diagnostics (e.g., medical imaging analysis, early disease prediction), accelerated drug discovery, personalized medicine, optimized clinical operations, and enhanced patient management and monitoring.\n",
      "*   **Key Benefits:** The integration of AI offers significant advantages such as improved accuracy and efficiency, enhanced patient outcomes, potential cost reductions, increased accessibility to care, and accelerated innovation in research and development.\n",
      "*   **Major Challenges:** Widespread AI adoption faces substantial hurdles, including issues with data quality, bias, and privacy, complex regulatory frameworks, integration with legacy systems, high implementation costs, and a shortage of technical expertise.\n",
      "*   **Ethical & Future Considerations:** Addressing profound ethical dilemmas like algorithmic bias, accountability, and the \"black box\" problem is crucial. The future emphasizes explainable AI (XAI), human-AI collaboration, and responsible, collaborative development to ensure equitable, transparent, and safe implementation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Generate a detailed report on {topic}.\")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"Generate a 5-point markdown summary for this text:\\n{text}.\")\n",
    "\n",
    "report_chain = prompt1 | llm | parser\n",
    "summary_chain = prompt2 | llm | parser\n",
    "\n",
    "pipeline = report_chain | RunnableLambda(lambda x: {\"text\": x}) | summary_chain\n",
    "\n",
    "response = pipeline.invoke({\"topic\": \"AI impact on Healthcare\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e6c87",
   "metadata": {},
   "source": [
    "### Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59a30f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_4o: Quantum computing harnesses the principles of quantum mechanics, such as superposition and entanglement, to perform complex calculations much faster than classical computers.\n",
      "gpt_4o_mini: Quantum computing is a revolutionary computational paradigm that uses the principles of quantum mechanics, such as superposition and entanglement, to perform complex calculations more efficiently than classical computers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain {topic} in one sentece.\")\n",
    "input_data = {\"topic\": \"quantum computing\"}\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    gpt_4o=prompt | ChatOpenAI(model=\"gpt-4o\") | StrOutputParser(),\n",
    "    gpt_4o_mini=prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser(),\n",
    ")\n",
    "\n",
    "response = parallel.invoke(input_data)\n",
    "\n",
    "for model_name, output in response.items():\n",
    "  print(f\"{model_name}: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9df36e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Category | AMD Ryzen 7 PRO 7840U | Intel Core i7-13700H |\n",
      "|----------|-------------|-------------|\n",
      "| Pros     | Based on the features provided, the two major pros of the AMD Ryzen 7 PRO 7840U processor are:<br>1. **Powerful Performance**: With 4 cores and 8 threads, a base clock speed of 2.7 GHz (up to 3.6 GHz boost), and integrated Radeon Vega graphics, this processor offers efficient multitasking, fast processing, smooth visuals, and graphics processing capabilities. It is well-suited for demanding business and professional tasks, providing a high level of performance.<br>2. **Enhanced Security and Efficiency**: The processor comes with AMD GuardMI technology for enhanced security features, ensuring data protection in professional environments. Additionally, the 15W TDP design offers energy efficiency, leading to longer battery life in laptops. This combination of security features and energy efficiency makes it a reliable choice for business users looking for both performance and data protection.   | 1. **High Performance**: The Intel Core i7-13700H with its 8 cores and 16 threads, along with a base clock speed of 2.30 GHz and a max turbo frequency of up to 4.60 GHz, offers exceptional performance for demanding tasks like gaming, content creation, and productivity. The 16 MB Intel Smart Cache also helps in improving overall performance by providing faster access to frequently used data.<br>2. **Enhanced Security**: The processor comes with advanced security features such as Intel Hardware Shield, Intel Trusted Execution Technology, and Intel Identity Protection Technology, which help in safeguarding the system against various security threats. This ensures that your data and system are protected while you engage in tasks that require high processing power.   |\n",
      "| Cons     | Based on the features provided, the two major cons of the AMD Ryzen 7 PRO 7840U processor are:<br>1. Limited Graphics Performance: While the integrated Radeon Vega graphics provide decent visuals and graphics processing capabilities, they may not be sufficient for demanding graphical tasks such as high-end gaming or professional graphic design work. Users requiring high-level graphics performance may need to invest in a dedicated graphics card.<br>2. Moderate Clock Speeds: Although the processor offers a base clock speed of 2.7 GHz and a max boost clock speed of 3.6 GHz, these speeds may not be as high as some other processors in the market. This could potentially impact the overall performance in tasks that require high processing speeds, such as video editing or 3D rendering. Users with demanding workloads may find the clock speeds of the Ryzen 7 PRO 7840U to be limiting in certain scenarios.   | Based on the features provided, the two major cons of the Intel Core i7-13700H processor are:<br>1. **Thermal Design Power (TDP)**: The TDP of 45W may lead to higher power consumption and heat generation, which could potentially result in increased thermal throttling under sustained heavy workloads. This may impact the overall performance and longevity of the laptop, especially in thin and light designs where thermal management is crucial.<br>2. **Integrated Graphics Performance**: While the Intel Iris Xe Graphics integrated in the processor offer decent performance for integrated graphics, they may not be sufficient for high-end gaming or professional graphics-intensive tasks. Users requiring more powerful graphics capabilities may need to opt for a laptop with dedicated graphics, which could limit the appeal of this processor for certain use cases.   |\n",
      "\n",
      "**Recommendation for LLMOps work with large model training:**\n",
      "content=\"Based on the user's requirement for LLMOps work with large model training, the Intel Core i7-13700H processor would be more suitable. \\n\\nThe Intel Core i7-13700H offers higher performance with its 8 cores and 16 threads, along with a base clock speed of 2.30 GHz and a max turbo frequency of up to 4.60 GHz. This level of processing power is essential for handling large model training tasks efficiently. Additionally, the 16 MB Intel Smart Cache helps in improving overall performance by providing faster access to frequently used data, which is crucial for handling large datasets.\\n\\nFurthermore, the advanced security features such as Intel Hardware Shield, Intel Trusted Execution Technology, and Intel Identity Protection Technology provide enhanced security for handling sensitive data during large model training tasks.\\n\\nWhile the TDP of 45W may lead to higher power consumption and heat generation, the user's focus on large model training tasks would benefit more from the higher performance capabilities of the Intel Core i7-13700H processor. Additionally, the integrated graphics performance may not be a major concern for LLMOps work, as the primary focus is on processing power rather than graphics performance.\\n\\nOverall, the Intel Core i7-13700H processor would be a more suitable choice for the user's requirements of LLMOps work with large model training due to its high performance capabilities and advanced security features.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 282, 'prompt_tokens': 752, 'total_tokens': 1034, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CdauZTqpuzp9uJ4I2x8KLxT0pwiMe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--ead8cba7-77ec-415e-a859-13fddee4af8e-0' usage_metadata={'input_tokens': 752, 'output_tokens': 282, 'total_tokens': 1034, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt template to get features\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert technology product reviewer.\"),\n",
    "    (\"human\", \"List the main features of the product {product_name}.\")\n",
    "])\n",
    "\n",
    "# Pros & Cons templates\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"Given these features: {features}, list the 2 major pros of these features.\")\n",
    "    ])\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"Given these features: {features}, list the 2 major cons of these features.\")\n",
    "    ])\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "# Build pros & cons chains\n",
    "pros_chain = RunnableLambda(lambda x: analyze_pros(x)) | llm | parser\n",
    "cons_chain = RunnableLambda(lambda x: analyze_cons(x)) | llm | parser\n",
    "\n",
    "# Pipeline to process a single product\n",
    "def product_review_pipeline(product_name):\n",
    "    return (\n",
    "        prompt_template\n",
    "        | llm\n",
    "        | parser\n",
    "        | RunnableParallel({\"pros\": pros_chain, \"cons\": cons_chain})\n",
    "        | RunnableLambda(lambda x: combine_pros_cons(x[\"pros\"], x[\"cons\"]))\n",
    "    ).invoke({\"product_name\": product_name})\n",
    "\n",
    "# Compare two products and recommend for a specific work request in Markdown\n",
    "def clean_text(text):\n",
    "    # Remove extra prefixes, newlines, and keep bullet points\n",
    "    return text.replace(\"\\n\\n\", \"\\n\").replace(\"\\n\", \"<br>\")\n",
    "\n",
    "def compare_and_recommend_md(product_1, product_2, work_request):\n",
    "    review_1 = product_review_pipeline(product_1)\n",
    "    review_2 = product_review_pipeline(product_2)\n",
    "\n",
    "    # Split Pros and Cons\n",
    "    pros_1, cons_1 = review_1.split(\"Cons:\")\n",
    "    pros_2, cons_2 = review_2.split(\"Cons:\")\n",
    "\n",
    "    pros_1 = clean_text(pros_1.replace(\"Pros:\", \"\").strip())\n",
    "    cons_1 = clean_text(cons_1.strip())\n",
    "    pros_2 = clean_text(pros_2.replace(\"Pros:\", \"\").strip())\n",
    "    cons_2 = clean_text(cons_2.strip())\n",
    "\n",
    "    # Recommendation prompt\n",
    "    recommendation_prompt = f\"\"\"\n",
    "    Reviews:\n",
    "    {product_1} Pros: {pros_1} Cons: {cons_1}\n",
    "    {product_2} Pros: {pros_2} Cons: {cons_2}\n",
    "\n",
    "    The user wants a laptop for: {work_request}.\n",
    "    Recommend which laptop is more suitable and explain why.\n",
    "    \"\"\"\n",
    "    recommendation = llm.invoke(recommendation_prompt)\n",
    "\n",
    "    # Markdown table with separate Pros and Cons columns\n",
    "    md_table = f\"\"\"\n",
    "| Category | {product_1} | {product_2} |\n",
    "|----------|-------------|-------------|\n",
    "| Pros     | {pros_1}   | {pros_2}   |\n",
    "| Cons     | {cons_1}   | {cons_2}   |\n",
    "\n",
    "**Recommendation for {work_request}:**\n",
    "{recommendation}\n",
    "\"\"\"\n",
    "    return md_table\n",
    "\n",
    "# Example usage\n",
    "product_1 = \"AMD Ryzen 7 PRO 7840U\"\n",
    "product_2 = \"Intel Core i7-13700H\"\n",
    "work_request = \"LLMOps work with large model training\"\n",
    "\n",
    "comparison_md = compare_and_recommend_md(product_1, product_2, work_request)\n",
    "print(comparison_md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc031cc",
   "metadata": {},
   "source": [
    "### Conditional Chains or Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89708491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " # Formal:\n",
      " Quantum computing is a computing paradigm that utilizes the principles of quantum mechanics to perform operations on data. Unlike classical computers, which process information in binary form (0s and 1s), quantum computers use quantum bits, or qubits, which can exist in multiple states simultaneously due to the principle of superposition. This allows quantum computers to store and process a vast amount of information in parallel, potentially making them significantly more powerful than classical computers for certain types of computations. Quantum computing leverages quantum entanglement and quantum interference to perform operations such as quantum parallelism and quantum teleportation. While still a developing field, quantum computing has the potential to revolutionize a wide range of industries, including cryptography, drug discovery, and artificial intelligence.\n",
      "\n",
      "\n",
      " # Informal:\n",
      " Quantum computing is like having a super smart, super fast computer that can do a million things at once... but only if you can figure out the secret password to unlock its full potential. It's like trying to teach a cat to do calculus - incredibly powerful, but also mind-bogglingly complex and unpredictable. So basically, it's like having a magical unicorn computer that can solve all your problems, as long as you speak its very specific, very strange language. Good luck with that!\n",
      "\n",
      "\n",
      " # Default:\n",
      " Quantum computing is a computational model that utilizes the principles of quantum mechanics, specifically superposition and entanglement, to process and store information. Unlike classical computing, which relies on bits to represent information as either 0 or 1, quantum computing uses quantum bits, or qubits, which can exist in a state of superposition, meaning they can represent both 0 and 1 simultaneously.\n",
      "\n",
      "Furthermore, qubits can also be entangled, meaning the state of one qubit can be dependent on the state of another, even if they are physically separated. This allows quantum computers to perform certain calculations much faster and more efficiently than classical computers, particularly for problems involving complex algorithms or large datasets.\n",
      "\n",
      "Although quantum computing is still in its early stages of development and has not yet reached practical applications for most industries, it holds the potential to revolutionize fields such as cryptography, optimization, and machine learning by solving problems that are currently intractable with classical computing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#2 prompts\n",
    "formal_prompt = ChatPromptTemplate.from_template(\"Explain {topic} in a formal way.\")\n",
    "informal_prompt = ChatPromptTemplate.from_template(\"Explain {topic} in a humorous way.\")\n",
    "\n",
    "#\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "#\n",
    "formal_chain = formal_prompt | model | parser\n",
    "informal_chain = informal_prompt | model | parser\n",
    "\n",
    "#\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: x[\"style\"] == \"formal\", formal_chain),\n",
    "    (lambda x: x[\"style\"] == \"informal\", informal_chain),\n",
    "    (formal_chain) #default\n",
    ")\n",
    "\n",
    "result1 = branch.invoke({\"topic\": \"quantum computing\", \"style\": \"formal\"})\n",
    "result2 = branch.invoke({\"topic\": \"quantum computing\", \"style\": \"informal\"})\n",
    "result3= branch.invoke({\"topic\": \"quantum computing\", \"style\": \"random\"})\n",
    "\n",
    "print(f\"\\n\\n # Formal:\\n {result1}\")\n",
    "print(f\"\\n\\n # Informal:\\n {result2}\")\n",
    "print(f\"\\n\\n # Default:\\n {result3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec5d6687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Customer Name],\n",
      "\n",
      "Thank you for taking the time to share your thoughts about our product. We appreciate your honesty and understand that making a decision can sometimes take time.\n",
      "\n",
      "If you have any specific concerns or questions about the product, we would be more than happy to address them. Our goal is to ensure that you have all the information you need to feel confident in your choice. Perhaps we can provide additional resources, answer any questions, or offer a demonstration to help clarify your concerns.\n",
      "\n",
      "We value your feedback and are here to support you. Please feel free to reach out to us directly if there‚Äôs anything we can do to assist you further.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Your Company]  \n",
      "[Contact Information]  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define prompt template for different feedback types\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Generate a thank you note for this positive feedback: {feedback}\"),\n",
    "])\n",
    "\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Generate a response addressing this negative feedback: {feedback}\"),\n",
    "])\n",
    "\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Generate a request for more details for this neutral feedback: {feedback}\"),\n",
    "])\n",
    "\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Generate a message to escalate the feedback to the human agent: {feedback}\"),\n",
    "])\n",
    "\n",
    "classification_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"Classify the following feedback into one of the categories 'positive', 'negative', or 'escalate': {feedback}\"),\n",
    "])\n",
    "\n",
    "# define the runnable branched for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (lambda x: \"positive\" in x, positive_feedback_template | llm | parser),\n",
    "    (lambda x: \"negative\" in x, negative_feedback_template | llm | parser),\n",
    "    (lambda x: \"neutral\" in x, neutral_feedback_template | llm | parser),\n",
    "    escalate_feedback_template | llm | parser\n",
    ")\n",
    "\n",
    "classification_chain = classification_template | llm | parser\n",
    "\n",
    "pipeline = classification_chain | branches\n",
    "\n",
    "\n",
    "# review = \"The product is terrible. It broke just after one use and the quality is very poor.\"\n",
    "# review = \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "review = \"The product is okay. It works as expected, nothing exceptional.\"\n",
    "review = \"I am not sure about the product yet.\"\n",
    "\n",
    "result = pipeline.invoke({\"feedback\": review})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1504deb",
   "metadata": {},
   "source": [
    "### SQL Database Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07eff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.utilities import SQLDatabase\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.runnables import SQLDatabaseChain\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# # Example Postgres URI\n",
    "# db_uri = userdata.get(\"postgresql_uri\") #\"postgresql+psycopg2://postgres:yourpassword@localhost:5432/mydatabase\"\n",
    "# print(db_uri)\n",
    "# # db = SQLDatabase.from_uri(db_uri)\n",
    "\n",
    "# # print(db.get_usable_table_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03783c09",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832808fc",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "It stores the coversation history. For every query, it sends the whole previous discussions (tokens) to LLM API. This can have a significant cost impact as API costs are based on number of tokens process and also the latency impact as coversation grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1d3e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConversationChain' from 'langchain_community.chains' (/home/azureuser/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_community/chains/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LLM\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ConversationChain' from 'langchain_community.chains' (/home/azureuser/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/langchain_community/chains/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Memory (legacy chain memory)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "# Conversation chain (legacy)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Make a prediction\n",
    "response = conversation.predict(input=\"Hi there! My name is Andrew\")\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I am good. I am from London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Can you suggest me some nearby places to visit in weekend?\")\n",
    "##Everything in green is from memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
