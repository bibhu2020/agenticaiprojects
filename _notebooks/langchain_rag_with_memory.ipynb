{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e485cfa",
   "metadata": {},
   "source": [
    "# RAG with Short-term and Long-term Memory\n",
    "Building a conversational RAG Chatbot with STM and LTM using Langchain,ChormaDB and OpenAI\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- Maintain conversations across multiple turns\n",
    "\n",
    "- Recall information from previous interactions\n",
    "\n",
    "- Retrieve domain-specific knowledge\n",
    "\n",
    "- Scale to long-term personalized interactions\n",
    "\n",
    "To achieve this, we designed a Conversational Retrieval-Augmented Generation (RAG) System equipped with both:\n",
    "\n",
    "- Short-Term Memory (STM) for multi-turn dialogue\n",
    "\n",
    "- Vector-Based Long-Term Memory (LTM) for storing persistent knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain>=1.0.7 langchain-community>=0.4.1 langchain-openai>=1.0.3 langchain_groq>=1.0.1 langchain_google_genai>=3.0.3 langchain-chroma>=1.0.0 langchain-text-splitters>=1.0.0 html2text>=2025.4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9513bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "import os\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "\n",
    "print(f\"answer is: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68240c",
   "metadata": {},
   "source": [
    "## Build a Chroma VectordB\n",
    "We will scrape web pages and store the embeddings in the vectordB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723fe794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load webpage\n",
    "urls = [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "# HTML ‚Üí text\n",
    "html2text = Html2TextTransformer()\n",
    "docs = html2text.transform_documents(docs)\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# Vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8345882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 matching chunks:\n",
      "\n",
      "[1] Content: Proof-of-Concept Examples Challenges Citation References Building agents with LLM (large language mo...\n",
      "\n",
      "[1] Metadata: {'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}...\n",
      "=================================================================\n",
      "\n",
      "[2] Content: API calls to solve it. Case Studies# Scientific Discovery Agent# ChemCrow (Bran et al. 2023) is a do...\n",
      "\n",
      "[2] Metadata: {'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}...\n",
      "=================================================================\n",
      "\n",
      "[3] Content: LLM Powered Autonomous Agents | Lil'Log Lil'Log | Posts Archive Search Tags FAQ LLM Powered Autonomo...\n",
      "\n",
      "[3] Metadata: {'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}...\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# quick simillarity search test\n",
    "results = vectorstore.similarity_search(\"What is a LLM agent?\", k=3)\n",
    "print(\"\\nüîç Top 3 matching chunks:\")\n",
    "# results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"\\n[{i+1}] Metadata: {doc.metadata}...\")\n",
    "    # print(f\"\\n[{i+1}] Simillarity Score: {doc}...\")\n",
    "    print(f\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9a7dc",
   "metadata": {},
   "source": [
    "## Define Coversational Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d96601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful Q&A assistant. Use the context below and the conversation history\n",
    "to answer the user's last question. If the answer is not in the context, say so.\n",
    "\n",
    "Conversation history:\n",
    "{chat_history}\n",
    "\n",
    "Retrieved context:\n",
    "{context}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366e299",
   "metadata": {},
   "source": [
    "## Define RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25274ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#Retriever + Formatting Pipeline\n",
    "from operator import itemgetter\n",
    "\n",
    "def format_docs(docs):\n",
    "    # convert list of Document objects ‚Üí string\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        # send ONLY the question string ‚Üí retriever ‚Üí format docs\n",
    "        \"context\": itemgetter(\"question\") | retriever | RunnableLambda(format_docs),\n",
    "\n",
    "        # send the question string into the prompt\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "\n",
    "        # send chat history array as string (RunnableWithMessageHistory injects this)\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416404a9",
   "metadata": {},
   "source": [
    "## Define Multi-turn Convesation with Short-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655340ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# in-memory session store\n",
    "store = {}\n",
    "\n",
    "def get_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversation_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"question\",        # the new user input\n",
    "    history_messages_key=\"chat_history\"   # pipeline expects {chat_history}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19df68",
   "metadata": {},
   "source": [
    "## Test the Short-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- TURN 1 ----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/ws/agenticaiprojects/.venv/lib/python3.12/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1: content='The context does not provide a specific definition for an AI agent. However, based on general knowledge, an AI agent can be described as a system or algorithm that employs artificial intelligence techniques to perform tasks in an autonomous manner, often making decisions based on its environment and goals. AI agents can plan, learn from experience, and interact with their surroundings, utilizing various functionalities such as memory and tool use.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 456, 'total_tokens': 535, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegbPrcQMxx0grTEph2vJVoRHkwAe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--7f90791f-21f8-4dac-9b89-e114e8c9ec7c-0' usage_metadata={'input_tokens': 456, 'output_tokens': 79, 'total_tokens': 535, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "---------------- TURN 2 ----------------\n",
      "A2: content='LLM-based agents utilize large language models (LLMs) as their core controllers to perform tasks autonomously. They function by employing planning, memory, and tool use to decompose tasks and generate solutions. For instance, in a system like ChemCrow, an LLM is augmented with expert-designed tools to handle specific tasks in fields such as organic synthesis and drug discovery. The LLM processes user requests to determine which tasks to delegate to various expert models, often presenting the task in a multiple-choice format. This allows the LLM to efficiently distribute tasks and leverage its capabilities as a general problem solver.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 843, 'total_tokens': 963, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegbU7A8MOLH1YEyugjMt2DpHArsP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--a89fc166-f913-4f9f-9c3a-451b69b57f0a-0' usage_metadata={'input_tokens': 843, 'output_tokens': 120, 'total_tokens': 963, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "---------------- TURN 3 ----------------\n",
      "A3: content='Yes, your first question was \"What is an AI agent?\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 1285, 'total_tokens': 1298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegbY4ckd71Y90tOXSJ7tEMB36UVP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--3e9be7d4-6608-42c4-ada1-0fcd283b96df-0' usage_metadata={'input_tokens': 1285, 'output_tokens': 13, 'total_tokens': 1298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "---------------- TURN 4 ----------------\n",
      "A4: content='Sure! Here\\'s a summary of our discussion so far:\\n\\n1. **AI Agent Definition**: An AI agent is a system or algorithm that uses artificial intelligence techniques to perform tasks autonomously, making decisions based on its environment and goals.\\n\\n2. **LLM-based Agents**: These agents utilize large language models (LLMs) as their core controllers, employing planning, memory, and tool use to decompose tasks and generate solutions. For example, ChemCrow uses an LLM augmented with specific tools for tasks in organic synthesis and drug discovery.\\n\\n3. **Memory of First Question**: I confirmed that your first question was, \"What is an AI agent?\"\\n\\nIf you have any further questions or need more details, feel free to ask!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 1601, 'total_tokens': 1750, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegbaoNbg726cjO8u11vUX4aHKdML', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--56440c01-abfc-44b8-9a63-6c53fe4cb66d-0' usage_metadata={'input_tokens': 1601, 'output_tokens': 149, 'total_tokens': 1750, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "---------------- TURN 5 ----------------\n",
      "A5: content=\"Based on our discussion, you might consider learning more about specific applications of LLM-based agents, such as how they utilize planning, memory, and tool use in practical scenarios, especially in fields like organic synthesis and drug discovery (as seen in systems like ChemCrow). Understanding the underlying principles of large language models, their capabilities, and their limitations could also be beneficial. Additionally, exploring the concepts of modular and neuro-symbolic architectures, like MRKL systems, might provide deeper insights into advanced AI systems. If you're interested in hands-on experience, consider experimenting with tools or frameworks that utilize these models for real-world tasks.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 2062, 'total_tokens': 2185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1024}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegbgZQTWr65RhqOzqKL3uNIQEMgs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--dbea08d4-168c-4da5-93e2-1e842c62585c-0' usage_metadata={'input_tokens': 2062, 'output_tokens': 123, 'total_tokens': 2185, 'input_token_details': {'audio': 0, 'cache_read': 1024}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "---------------- TURN 6 ----------------\n",
      "A6: content='Your second question was, \"How do LLM-based agents work?\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 2488, 'total_tokens': 2502, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegblD9Qv3YqjVcauE6ct5CKYQQwc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--adcfb7ea-74b0-419a-a7d5-23caeee9c599-0' usage_metadata={'input_tokens': 2488, 'output_tokens': 14, 'total_tokens': 2502, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "session_id = \"memory_test_01\"\n",
    "\n",
    "print(\"\\n---------------- TURN 1 ----------------\")\n",
    "response1 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"What is an AI agent?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A1:\", response1)\n",
    "\n",
    "print(\"\\n---------------- TURN 2 ----------------\")\n",
    "response2 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"How do LLM-based agents work?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A2:\", response2)\n",
    "\n",
    "print(\"\\n---------------- TURN 3 ----------------\")\n",
    "response3 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"Did you remember my first question?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A3:\", response3)\n",
    "\n",
    "print(\"\\n---------------- TURN 4 ----------------\")\n",
    "response4 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"Summarize everything we have discussed so far.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A4:\", response4)\n",
    "\n",
    "print(\"\\n---------------- TURN 5 ----------------\")\n",
    "response5 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"Based on our discussion, what should I learn next?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A5:\", response5)\n",
    "\n",
    "print(\"\\n---------------- TURN 6 ----------------\")\n",
    "response6 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"What was my second question again?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A6:\", response6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd69332",
   "metadata": {},
   "source": [
    "## Continious Chat with Short-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddcdc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chatbot Ready! (type 'exit' or 'quit' to stop)\n",
      "-----------------------------------------------------\n",
      "Bot: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 421, 'total_tokens': 430, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CegdBGFQQM6hKFXP6BhPtFgJezaV1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--6a979394-896b-4c27-8ade-cba955f89fd3-0' usage_metadata={'input_tokens': 421, 'output_tokens': 9, 'total_tokens': 430, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you! How can I help you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 780, 'total_tokens': 808, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegdLtguCY2JIMTUAzLCQ61HdIeeO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--cf41bda2-3c96-4a65-a386-0ed20d19d52c-0' usage_metadata={'input_tokens': 780, 'output_tokens': 28, 'total_tokens': 808, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content=\"I'm just a computer program and don't have a name like humans do. But you can call me whatever you like! How can I assist you today, Bibhu?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 1092, 'total_tokens': 1125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-Cegdao1yp7apgUEpSxk2iIEWf4exy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--5f6b8f26-1c9f-4f79-a9f0-3a0e6a598d81-0' usage_metadata={'input_tokens': 1092, 'output_tokens': 33, 'total_tokens': 1125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content=\"I'm just a computer program and don't have a physical location like humans do. However, I'm here to assist you with any questions you have!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 1441, 'total_tokens': 1469, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegdnX0UQm9Xcmsc6OboyV47b3DRM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--cdc587cd-dc35-4fdb-bc19-fca7cd074aef-0' usage_metadata={'input_tokens': 1441, 'output_tokens': 28, 'total_tokens': 1469, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content=\"I'm sorry, but I don't have access to real-time weather information or specific local data. You may want to check a weather website or app for the latest updates on the weather in Dallas.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 1769, 'total_tokens': 1807, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-Cege2P63jmesWgyaabbSR085rVWGA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--8f80d252-3f0b-4ddc-bf81-23a3cd7cc42a-0' usage_metadata={'input_tokens': 1769, 'output_tokens': 38, 'total_tokens': 1807, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content=\"I'm sorry, but I don't have access to real-time weather information or specific local data. You may want to check a weather website or app for the latest updates on the weather in Dallas.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 2091, 'total_tokens': 2129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegeI2VC2mtovWphrDHGFa5Wv9xcA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--e7284eed-1634-4e1f-b670-424c72faa92c-0' usage_metadata={'input_tokens': 2091, 'output_tokens': 38, 'total_tokens': 2129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content='The context provided does not contain information about Aurangzeb. Would you like me to give you a brief overview based on my knowledge?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 2456, 'total_tokens': 2484, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegeaoLH2OHrg6kUikQAZwmtYQMXF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0df68b4d-f28a-4a53-9a0d-1951d076c78f-0' usage_metadata={'input_tokens': 2456, 'output_tokens': 28, 'total_tokens': 2484, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content='The context provided does not contain information about Aurangzeb. Would you like me to give you a brief overview based on my knowledge?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 2786, 'total_tokens': 2814, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegejOpxGNaLSto0IHmmcPPg1A4vZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--626a2097-5ba2-4011-b734-99a8c7c574fa-0' usage_metadata={'input_tokens': 2786, 'output_tokens': 28, 'total_tokens': 2814, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content='An agent, in the context of the provided information, refers to an entity (such as a computer program or a virtual character) that can plan, react, and interact within an environment. Agents take into account relationships and observations to make decisions and perform tasks. They operate using a generative architecture, which allows them to engage in complex behaviors, such as information diffusion and relationship maintenance. This framework supports the autonomous design and execution of tasks, especially in simulations or applications that require interaction and adaptability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 3069, 'total_tokens': 3168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1920}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegexEkfALwweS9ivtLCxCHOtrN2S', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1a8e481a-337f-4adc-b6e2-a62b357b1b03-0' usage_metadata={'input_tokens': 3069, 'output_tokens': 99, 'total_tokens': 3168, 'input_token_details': {'audio': 0, 'cache_read': 1920}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: content='An agent, in the context of the provided information, refers to an entity (such as a computer program or a virtual character) that can plan, react, and interact within an environment. Agents take into account relationships and observations to make decisions and perform tasks. They operate using a generative architecture, which allows them to engage in complex behaviors, such as information diffusion and relationship maintenance. This framework supports the autonomous design and execution of tasks, especially in simulations or applications that require interaction and adaptability.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 3466, 'total_tokens': 3565, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-Cegf5iO35GG9bqyOvebZHTjrkPSh5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--6cf579b9-f06b-429c-baff-e8cbfa3f46ba-0' usage_metadata={'input_tokens': 3466, 'output_tokens': 99, 'total_tokens': 3565, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def chat_with_rag(session_id=\"chat_user\"):\n",
    "    print(\"RAG Chatbot Ready! (type 'exit' or 'quit' to stop)\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # invoke the chain with memory\n",
    "        response = conversation_rag_chain.invoke(\n",
    "            {\"question\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "\n",
    "        print(f\"Bot: {response}\\n\")\n",
    "\n",
    "\n",
    "# start chatbot\n",
    "chat_with_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1511b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chatbot Ready! (type 'exit' or 'quit' to stop)\n",
      "-----------------------------------------------------\n",
      "Bot: content='Your name is Bibhu.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 3917, 'total_tokens': 3923, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2944}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegfovzEfYL5sCOoxVYIHmT0XkHjR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--386d652e-1196-4942-a7be-ea0fd8cbe502-0' usage_metadata={'input_tokens': 3917, 'output_tokens': 6, 'total_tokens': 3923, 'input_token_details': {'audio': 0, 'cache_read': 2944}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "session_id=\"chat_user\" #\"run--8eb56645-b013-4d1e-98b2-66e1f123dd86-0\"\n",
    "\n",
    "chat_with_rag(session_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30cb19",
   "metadata": {},
   "source": [
    "## Build Long-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8b672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7368/1677433964.py:10: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  ltm_vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Create a Chroma dB as Long Term Memory Store\n",
    "#Create vector Store\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "ltm_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a separate Chroma DB for long-term memory\n",
    "ltm_vectorstore = Chroma(\n",
    "    collection_name=\"long_term_memory\",\n",
    "    embedding_function=ltm_embeddings\n",
    ")\n",
    "\n",
    "# Long-term memory retriever\n",
    "ltm_retriever = ltm_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb6928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Long Term Memory Updation\n",
    "\n",
    "def save_to_ltm(question, answer):\n",
    "    \"\"\"Store important conversational info as long-term memory.\"\"\"\n",
    "    text = f\"User asked: {question}\\nAssistant answered: {answer}\"\n",
    "    ltm_vectorstore.add_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f58839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define RAG Pipeline with Long-term Memory\n",
    "\n",
    "def join_text(data):\n",
    "    return \"\\n\\n\".join([d.page_content for d in data])\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"rag_context\": itemgetter(\"question\") | retriever | RunnableLambda(format_docs),\n",
    "        \"ltm_context\": itemgetter(\"question\") | ltm_retriever | RunnableLambda(format_docs),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant. Use ALL sources below:\n",
    "\n",
    "1. Short-term chat history:\n",
    "{chat_history}\n",
    "\n",
    "2. Long-term memory (LTM):\n",
    "{ltm_context}\n",
    "\n",
    "3. Retrieved knowledge (RAG):\n",
    "{rag_context}\n",
    "\n",
    "Answer the final user question:\n",
    "{question}\n",
    "\"\"\")\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6de518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_fn(message, history):\n",
    "    session_id = \"memory_test_02\"  #New with LMT\n",
    "\n",
    "    answer = conversation_rag_chain.invoke(\n",
    "        {\"question\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "\n",
    "    # Save important info into long-term memory\n",
    "    save_to_ltm(message, answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e73a969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TURN 1: Store a personal fact in LTM ===\n",
      "A1: content='The retrieved context does not contain information related to birthdays or personal celebrations. Sorry, I cannot provide an answer to your statement about your birthday.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 432, 'total_tokens': 460, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiS4T1t9mVvXPrGpYNrfJ3B5XIS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1f463811-b603-4531-8cd5-54ebc0896d43-0' usage_metadata={'input_tokens': 432, 'output_tokens': 28, 'total_tokens': 460, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "=== TURN 2‚Äì5: Distract the model with unrelated queries ===\n",
      "\n",
      "--- TURN 2: Explain the difference between supervised and unsupervised learning. ---\n",
      "A2: content='The retrieved context does not contain information related to the difference between supervised and unsupervised learning. Sorry, I cannot provide an answer to your question.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 836, 'total_tokens': 866, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiUyhXYlPAeFymOLSFeQMrrGOyW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--9462e553-e889-4731-bbc5-a5c5c396d32b-0' usage_metadata={'input_tokens': 836, 'output_tokens': 30, 'total_tokens': 866, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "--- TURN 3: What is the capital of France? ---\n",
      "A3: content='The retrieved context does not contain information related to the capital of France. Sorry, I cannot provide an answer to your question.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1167, 'total_tokens': 1192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiV4GkpoBCuS1rWFxWG9KkmGBVo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--fa4fd4ac-fbfb-4472-a10d-9f7528564f77-0' usage_metadata={'input_tokens': 1167, 'output_tokens': 25, 'total_tokens': 1192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "--- TURN 4: Tell me something about reinforcement learning. ---\n",
      "A4: content='The retrieved context does not contain specific information about reinforcement learning. Sorry, I cannot provide an answer to your question.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 1505, 'total_tokens': 1528, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiWQYgDV3OUe7K9fpkm4piKDYvb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--931b5471-d541-4478-911f-3fc59f84339d-0' usage_metadata={'input_tokens': 1505, 'output_tokens': 23, 'total_tokens': 1528, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "--- TURN 5: What is backpropagation? ---\n",
      "A5: content='The retrieved context does not contain specific information about backpropagation. Sorry, I cannot provide an answer to your question.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 1755, 'total_tokens': 1779, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiYIFa3WJsKP6iwTDXGlo239Njq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--b236dd78-dbe0-440a-8e43-a647c818a691-0' usage_metadata={'input_tokens': 1755, 'output_tokens': 24, 'total_tokens': 1779, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "=== TURN 6: Critical: Ask LTM-dependent question ===\n",
      "A6: content='Your birthday is on 14th October.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 2004, 'total_tokens': 2013, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiZTfMniAy3VQtgq2Ox2eSz9brc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--ddd55860-159d-423c-b93d-e07bd8ccdfc9-0' usage_metadata={'input_tokens': 2004, 'output_tokens': 9, 'total_tokens': 2013, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "=== TURN 7: Even stronger LTM recall test ===\n",
      "A7: content='You mentioned that your birthday is on 14th October.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 2375, 'total_tokens': 2387, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegiaCUGkUctGT8Fr3ENH5GfTLhPx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--0a2d33ae-64fc-4801-9943-895a7de26162-0' usage_metadata={'input_tokens': 2375, 'output_tokens': 12, 'total_tokens': 2387, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "=== TURN 8: Ask for reasoning using long-term memory ===\n",
      "A8: content='Your birthday is on 14th October, which makes your zodiac sign Libra.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 2636, 'total_tokens': 2652, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1920}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegibCXwFdZjIe8MYTShErgvNdabu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--2130827d-d769-408b-a7bf-74229d2cab84-0' usage_metadata={'input_tokens': 2636, 'output_tokens': 16, 'total_tokens': 2652, 'input_token_details': {'audio': 0, 'cache_read': 1920}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "=== TURN 9: Ask to explain how it remembered ===\n",
      "A9: content='I remember your birthday because it was specifically mentioned in our conversation. The memory of your birthday is retained even when the questions that follow are unrelated. This allows me to respond accurately when it comes up again.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 3016, 'total_tokens': 3057, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2176}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CegicrX54JbL9mN2P1ji9apKP768v', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--cfeb36f4-b288-45c5-bf49-989b53fb75de-0' usage_metadata={'input_tokens': 3016, 'output_tokens': 41, 'total_tokens': 3057, 'input_token_details': {'audio': 0, 'cache_read': 2176}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "session_id = \"long_term_test_01\"\n",
    "\n",
    "print(\"=== TURN 1: Store a personal fact in LTM ===\")\n",
    "response1 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"My birthday is on 14th October.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A1:\", response1)\n",
    "\n",
    "# Save to LTM\n",
    "save_to_ltm(\"My birthday is on 14th October.\", response1)\n",
    "\n",
    "\n",
    "print(\"\\n=== TURN 2‚Äì5: Distract the model with unrelated queries ===\")\n",
    "unrelated_questions = [\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me something about reinforcement learning.\",\n",
    "    \"What is backpropagation?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(unrelated_questions, start=2):\n",
    "    print(f\"\\n--- TURN {i}: {q} ---\")\n",
    "    r = conversation_rag_chain.invoke(\n",
    "        {\"question\": q},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(f\"A{i}:\", r)\n",
    "    # these do NOT go to LTM ‚Äî only distract\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== TURN 6: Critical: Ask LTM-dependent question ===\")\n",
    "response6 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"When is my birthday?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A6:\", response6)\n",
    "\n",
    "\n",
    "print(\"\\n=== TURN 7: Even stronger LTM recall test ===\")\n",
    "response7 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"Earlier you learned a fact about my personal life. What was it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A7:\", response7)\n",
    "\n",
    "\n",
    "print(\"\\n=== TURN 8: Ask for reasoning using long-term memory ===\")\n",
    "response8 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"Since my birthday is on that date, what zodiac sign am I?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A8:\", response8)\n",
    "\n",
    "\n",
    "print(\"\\n=== TURN 9: Ask to explain how it remembered ===\")\n",
    "response9 = conversation_rag_chain.invoke(\n",
    "    {\"question\": \"How did you remember my birthday even after many unrelated questions?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"A9:\", response9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
