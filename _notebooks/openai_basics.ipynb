{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0533c0db",
   "metadata": {},
   "source": [
    "# Async Python (asyncio package)\n",
    "\n",
    "Async Python is light-weight alternative to threading or multiprocessing. \n",
    "\n",
    "    - \"async def function()\" is called coroutine. \n",
    "    - coroutine does not execute immediately - it returns a coroutine object.\n",
    "    - It waits for execution within an event loop.\n",
    "    - event loop executes the coroutine when its turn come up.\n",
    "    \n",
    "```python\n",
    "async def do_something() -> str:\n",
    "    # do something\n",
    "    return \"done!\"\n",
    "\n",
    "result = await do_something()\n",
    "```\n",
    "\n",
    "```python\n",
    "# It returns result list\n",
    "results = await asyncio.gather(\n",
    "    do_something(),\n",
    "    do_something2(),\n",
    "    do_something3()\n",
    ")\n",
    "```\n",
    "\n",
    "# Core Concepts\n",
    "- **Agents**, which are LLMs equipped with instructions (or system prompts) and tools.\n",
    " (OpenAI Agents SDK adopts a more modular and flexible approach)\n",
    "\n",
    "- **Handoffs**, which allow agents to delegate to other agents for specific tasks\n",
    "\n",
    "- **Guardrails**, which enable validation of agent inputs and outputs\n",
    "\n",
    "- **Sessions**, which automatically maintains conversation history across agent runs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16870888",
   "metadata": {},
   "source": [
    "# Cheetsheet for Agentic Frameworks\n",
    "\n",
    "## Setting up LLM Keys\n",
    "```python\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "```\n",
    "\n",
    "## OpenAI SDK\n",
    "\n",
    "### Chatting with OpenAI\n",
    "\n",
    "```python\n",
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n",
    "```\n",
    "\n",
    "### Chatting with Gemenai\n",
    "\n",
    "```python\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "```\n",
    "\n",
    "### Chatting with Groq\n",
    "\n",
    "```python\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "```\n",
    "\n",
    "### Chatting with Ollama (llm running locally)\n",
    "\n",
    "```python\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "```\n",
    "\n",
    "--------\n",
    "\n",
    "## OpenAI SDK for Agents\n",
    "\n",
    "### Define an Agent with OpenAI LLM\n",
    "```python\n",
    "# Defining an Agent\n",
    "agent1 = Agent(\n",
    "    name=\"....\",                # give a name to the model\n",
    "    instructions=\".....\",       # provide the system instruction\n",
    "    model=\"gpt-4o-mini\",        # model name\n",
    "    tools=[],                   # optional. list of tools that you would like to link to the agent\n",
    "    handoffs=[],                # optional. list of agents that you would like to link to the agent\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Invoking an Agent\n",
    "```python\n",
    "with trace(\"tracing1\"):\n",
    "    result = await Runner.run(agent1, \"tell a joke about automation AI agents\")\n",
    "    print(result.final_output)\n",
    "\n",
    "# invoke multiple agents in parallel\n",
    "with trace(\"Parallel cold emails\"):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(sales_agent1, message),\n",
    "        Runner.run(sales_agent2, message),\n",
    "        Runner.run(sales_agent3, message),\n",
    "    ) \n",
    "\n",
    "outputs = [result.final_output for result in results]\n",
    "\n",
    "for output in outputs:\n",
    "    print(output + \"\\n\\n\"   \n",
    "```\n",
    "\n",
    "### Defining a tool\n",
    "```python\n",
    "# convert a function to a tool\n",
    "@funtion_tool\n",
    "def something(....):\n",
    "    \"\"\" document the function here \"\"\"\n",
    "    pass\n",
    "\n",
    "# convert an agent to a tool\n",
    "tool1 = agent1.as_tool(tool_name=\"agent1\", tool_description=description)\n",
    "tool2 = agent2.as_tool(tool_name=\"agent2\", tool_description=description)\n",
    "tool3 = agent3.as_tool(tool_name=\"agent3\", tool_description=description)\n",
    "\n",
    "tools = [tool1, tool2, tool3, something] #this becomes the array of tools that you can pass to an agent\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Define an Agent with non-OpenAI LLM\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "## OpenAI SDK for CrewAI"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
